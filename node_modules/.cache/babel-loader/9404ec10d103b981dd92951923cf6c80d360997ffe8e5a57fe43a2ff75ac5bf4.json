{"ast":null,"code":"/**\n * Service for handling interactions with Google's Gemini AI API\n */\nclass GeminiService {\n  /**\n   * Creates an instance of GeminiService.\n   * @param {string} apiKey - Your Google AI API key.\n   * @param {string} [modelName='gemini-2.5-flash-preview-04-17'] - The Gemini model to use.\n   *        Examples: 'gemini-1.5-pro-latest', 'gemini-1.5-flash-latest', 'gemini-pro'.\n   */\n  constructor(apiKey, modelName = 'gemini-2.0-flash') {\n    // Default to the latest Pro model\n    if (!apiKey) {\n      throw new Error(\"API key is required for GeminiService.\");\n    }\n    this.apiKey = apiKey;\n    this.modelName = modelName;\n    // Construct the base URL using the provided model name\n    this.baseUrl = `https://generativelanguage.googleapis.com/v1beta/models/${this.modelName}:generateContent`;\n    this.history = [];\n    this.characterProfile = null;\n    this.scenarioDetails = null;\n    console.log(`GeminiService initialized with model: ${this.modelName}`);\n  }\n\n  /**\n   * Initialize the service with character and scenario information\n   * @param {Object} characterProfile - The detailed character profile\n   * @param {Object} scenarioDetails - The scenario setup details\n   */\n  initialize(characterProfile, scenarioDetails) {\n    this.characterProfile = characterProfile;\n    this.scenarioDetails = scenarioDetails;\n    this.history = [];\n\n    // Create system message that sets up the context\n    const systemContext = this._createSystemContext();\n    // System instructions should generally be 'user' role for the *first* turn\n    // Then the model responds. However, Gemini's multi-turn chat expects alternating roles.\n    // For initial setup, placing it as 'model' might work if followed by user input,\n    // but often it's better to combine setup instructions into the first 'user' message\n    // or use the dedicated `system_instruction` field if the API version supports it.\n    // Let's keep your structure for now, but be mindful of this.\n    this.history.push({\n      role: 'model',\n      // Or potentially 'user' if it's the very first thing\n      parts: [{\n        text: systemContext\n      }]\n    });\n\n    // If there are roleplay instructions, add them as system context (as 'model' role again)\n    if (scenarioDetails.roleplayInstructions) {\n      // Combine instructions with initial context or place strategically.\n      // Adding multiple 'model' turns consecutively might confuse the structure.\n      // Let's merge it into the first system message or ensure a 'user' turn happens before the actual chat.\n      // For simplicity, let's append it to the initial context message:\n      const initialModelMessage = this.history[0];\n      initialModelMessage.parts[0].text += `\\n\\nRoleplay Instructions:\\n\\n${scenarioDetails.roleplayInstructions}`;\n\n      // Original logic (kept commented out for reference):\n      // this.history.push({\n      //   role: 'model',\n      //   parts: [{ text: `Roleplay Instructions:\\n\\n${scenarioDetails.roleplayInstructions}` }]\n      // });\n    }\n    // Add an initial user prompt to kick off the conversation after setup\n    // This helps establish the user/model turn structure\n    this.history.push({\n      role: 'user',\n      parts: [{\n        text: \"Okay, I understand the setup. Let's begin the roleplay.\"\n      }]\n    });\n    // Expect the model's first response acknowledging the start\n  }\n\n  /**\n   * Update roleplay instructions\n   * @param {string} instructions - New roleplay instructions\n   */\n  updateInstructions(instructions) {\n    // Find the *last* 'model' message that contains instructions, or add a new one.\n    // It's often better to send updates as a 'user' message instructing the model.\n    // Let's try sending it as a user directive.\n\n    const instructionUpdateText = `(System Note: Please adhere to the following updated roleplay instructions from now on, incorporating them naturally into your responses without explicitly mentioning this note: ${instructions})`;\n    this.history.push({\n      role: 'user',\n      // Send as a user message to instruct the model for the next turn\n      parts: [{\n        text: instructionUpdateText\n      }]\n    });\n\n    // The model's *next* response should implicitly follow these instructions.\n    // Avoid adding extra 'model' reminders as it pollutes the history.\n\n    // --- Original Logic (commented out) ---\n    // // Find any existing instruction message\n    // const existingInstructionIndex = this.history.findIndex(msg =>\n    //   msg.role === 'model' &&\n    //   msg.parts[0]?.text?.includes('Roleplay Instructions:')\n    // );\n    //\n    // // Create the instruction message\n    // const instructionMessage = {\n    //   role: 'model', // This can disrupt the turn flow if inserted arbitrarily\n    //   parts: [{ text: `Roleplay Instructions:\\n\\n${instructions}` }]\n    // };\n    //\n    // if (existingInstructionIndex !== -1) {\n    //   // Replace existing instructions\n    //   this.history[existingInstructionIndex] = instructionMessage;\n    // } else {\n    //   // Add new instructions after the system context\n    //   // Finding the right place to insert can be tricky. Appending might be safer.\n    //   this.history.splice(1, 0, instructionMessage); // Inserting might break user/model alternation\n    // }\n    //\n    // // Add a reminder to follow the new instructions (not visible to user) - Avoid this if possible\n    // this.history.push({\n    //   role: 'model',\n    //   parts: [{ text: 'Remember to follow the updated roleplay instructions provided above, but do not explicitly mention them in your responses.' }]\n    // });\n    // --- End Original Logic ---\n  }\n\n  /**\n   * Generate a generic response from Gemini based on a prompt (uses minimal history)\n   * @param {string} prompt - The prompt to send to Gemini\n   * @returns {Promise<string>} - The AI generated response\n   */\n  async generateGeneric(prompt) {\n    // This function seems intended for non-chat, single-turn generation.\n    // It should NOT use the main chat history.\n    try {\n      var _data$candidates$, _data$candidates$$con, _data$candidates$$con2, _data$promptFeedback2;\n      console.log(`Sending generic request to Gemini API (${this.modelName})`);\n      console.log('Using API URL:', this.baseUrl);\n\n      // Prepare contents for a single-turn request\n      const contents = [{\n        role: 'user',\n        parts: [{\n          text: prompt\n        }]\n      }];\n\n      // Add retry logic with exponential backoff (same as before)\n      let retries = 3;\n      let delay = 1000; // Start with 1 second delay\n      let success = false;\n      let data;\n      while (retries > 0 && !success) {\n        try {\n          const response = await fetch(`${this.baseUrl}?key=${this.apiKey}`, {\n            method: 'POST',\n            headers: {\n              'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n              contents: contents,\n              // Use the single prompt content\n              generationConfig: {\n                temperature: 0.7,\n                topK: 40,\n                topP: 0.95,\n                maxOutputTokens: 1000 // Adjust as needed\n              },\n              // Safety settings might also be relevant here if needed\n              safetySettings: [\n              // Keep safety settings consistent\n              {\n                category: \"HARM_CATEGORY_HARASSMENT\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }, {\n                category: \"HARM_CATEGORY_HATE_SPEECH\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }, {\n                category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                threshold: \"BLOCK_ONLY_HIGH\"\n              }, {\n                category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }]\n            })\n          });\n\n          // Check HTTP status (same as before)\n          if (!response.ok) {\n            const errorText = await response.text();\n            console.error('API error response (generic):', errorText);\n            // Throw specific errors (same as before)\n            if (response.status === 400) throw new Error(`API error: 400 - Bad Request. Check model name ('${this.modelName}') and parameters. Details: ${errorText}`);\n            if (response.status === 401) throw new Error(`API error: 401 - Unauthorized. Your API key may be invalid or expired.`);\n            if (response.status === 403) throw new Error(`API error: 403 - Forbidden. Your API key may not have access to '${this.modelName}' or you've exceeded quota.`);\n            if (response.status === 404) throw new Error(`API error: 404 - Not Found. The specified model '${this.modelName}' may not exist or the URL is incorrect.`);\n            if (response.status === 429) throw new Error(`API error: 429 - Too Many Requests. You've hit rate limits or quota restrictions.`);\n            throw new Error(`API error (generic): ${response.status} - ${errorText || 'Unknown error'}`);\n          }\n          data = await response.json();\n          // console.log('API response structure (generic):', JSON.stringify(data, null, 2).substring(0, 500) + '...'); // Optional: for debugging\n          success = true;\n        } catch (err) {\n          console.error(`Attempt ${4 - retries} failed (generic):`, err);\n          retries--;\n          if (retries > 0) {\n            console.log(`Retrying in ${delay}ms...`);\n            await new Promise(resolve => setTimeout(resolve, delay));\n            delay *= 2; // Exponential backoff\n          } else {\n            throw err; // Re-throw the last error if all retries failed\n          }\n        }\n      }\n\n      // Extract the model's response (same as before)\n      if (data.candidates && ((_data$candidates$ = data.candidates[0]) === null || _data$candidates$ === void 0 ? void 0 : (_data$candidates$$con = _data$candidates$.content) === null || _data$candidates$$con === void 0 ? void 0 : (_data$candidates$$con2 = _data$candidates$$con.parts) === null || _data$candidates$$con2 === void 0 ? void 0 : _data$candidates$$con2.length) > 0) {\n        // Safety check: Check for finishReason\n        if (data.candidates[0].finishReason === 'SAFETY') {\n          var _data$promptFeedback;\n          console.warn('Gemini response blocked due to safety settings.');\n          // Optionally check promptFeedback for block reason\n          if ((_data$promptFeedback = data.promptFeedback) !== null && _data$promptFeedback !== void 0 && _data$promptFeedback.blockReason) {\n            console.warn(`Safety block reason: ${data.promptFeedback.blockReason}`);\n          }\n          return \"[Response blocked due to safety settings]\";\n        }\n        return data.candidates[0].content.parts[0].text;\n      } else if ((_data$promptFeedback2 = data.promptFeedback) !== null && _data$promptFeedback2 !== void 0 && _data$promptFeedback2.blockReason) {\n        // Handle cases where the prompt itself was blocked\n        console.warn(`Gemini prompt blocked due to: ${data.promptFeedback.blockReason}`);\n        return `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]`;\n      } else {\n        console.error('Unexpected API response structure (generic):', data);\n        throw new Error('Invalid response format from Gemini API (generic)');\n      }\n    } catch (error) {\n      console.error('Error calling Gemini API (generic):', error);\n      // Don't add generic prompt/response to history\n      throw error;\n    }\n  }\n\n  /**\n   * Generate a response from Gemini based on user input and chat history\n   * @param {string} userMessage - The user's message content\n   * @param {string} messageType - Type of message (dialogue, action, thought)\n   * @returns {Promise<string>} - The AI generated response\n   */\n  async generateResponse(userMessage, messageType) {\n    try {\n      var _data$candidates$2, _data$candidates$2$co, _data$candidates$2$co2, _data$promptFeedback4;\n      // Add user message to history\n      this.history.push({\n        role: 'user',\n        parts: [{\n          text: `[${messageType.toUpperCase()}] ${userMessage}`\n        }]\n      });\n\n      // Ensure history alternates user/model roles correctly.\n      // If the last two roles are the same, it might indicate an issue.\n      // Basic check:\n      if (this.history.length >= 2 && this.history[this.history.length - 1].role === this.history[this.history.length - 2].role) {\n        console.warn(\"Potential history role alternation issue detected before API call.\");\n        // Depending on the cause, you might need to adjust `initialize` or `updateInstructions`\n      }\n\n      // Prepare the context window for Gemini (using the full history)\n      const contents = [...this.history];\n      console.log(`Sending roleplay request to Gemini API (${this.modelName})`);\n      // console.log('Using API URL:', this.baseUrl); // Already logged in constructor/generic\n      // console.log('Sending history:', JSON.stringify(contents, null, 2)); // Debugging: log history being sent\n\n      // Add retry logic with exponential backoff (same as before)\n      let retries = 3;\n      let delay = 1000;\n      let success = false;\n      let data;\n      while (retries > 0 && !success) {\n        try {\n          const response = await fetch(`${this.baseUrl}?key=${this.apiKey}`, {\n            method: 'POST',\n            headers: {\n              'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n              contents: contents,\n              // Send the chat history\n              generationConfig: {\n                temperature: 0.7,\n                // Adjust as needed for creativity vs coherence\n                topK: 40,\n                topP: 0.95,\n                maxOutputTokens: 800 // Adjust based on desired response length\n                // stopSequences: [] // Optional: sequences that stop generation\n              },\n              safetySettings: [\n              // Keep safety settings consistent\n              {\n                category: \"HARM_CATEGORY_HARASSMENT\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }, {\n                category: \"HARM_CATEGORY_HATE_SPEECH\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }, {\n                category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                threshold: \"BLOCK_ONLY_HIGH\"\n              },\n              // Be cautious with roleplay\n              {\n                category: \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                threshold: \"BLOCK_MEDIUM_AND_ABOVE\"\n              }]\n            })\n          });\n\n          // Check HTTP status (same as before)\n          if (!response.ok) {\n            const errorText = await response.text();\n            console.error('API error response (roleplay):', errorText);\n            // Throw specific errors (same as before)\n            if (response.status === 400) throw new Error(`API error: 400 - Bad Request. Check model name ('${this.modelName}'), history structure, or parameters. Details: ${errorText}`);\n            if (response.status === 401) throw new Error(`API error: 401 - Unauthorized. Your API key may be invalid or expired.`);\n            if (response.status === 403) throw new Error(`API error: 403 - Forbidden. Your API key may not have access to '${this.modelName}' or you've exceeded quota.`);\n            if (response.status === 404) throw new Error(`API error: 404 - Not Found. The specified model '${this.modelName}' may not exist or the URL is incorrect.`);\n            if (response.status === 429) throw new Error(`API error: 429 - Too Many Requests. You've hit rate limits or quota restrictions.`);\n            throw new Error(`API error (roleplay): ${response.status} - ${errorText || 'Unknown error'}`);\n          }\n          data = await response.json();\n          success = true;\n        } catch (err) {\n          console.error(`Attempt ${4 - retries} failed (roleplay):`, err);\n          retries--;\n          if (retries > 0) {\n            console.log(`Retrying in ${delay}ms...`);\n            await new Promise(resolve => setTimeout(resolve, delay));\n            delay *= 2;\n          } else {\n            throw err;\n          }\n        }\n      }\n\n      // Extract the model's response\n      if (data.candidates && ((_data$candidates$2 = data.candidates[0]) === null || _data$candidates$2 === void 0 ? void 0 : (_data$candidates$2$co = _data$candidates$2.content) === null || _data$candidates$2$co === void 0 ? void 0 : (_data$candidates$2$co2 = _data$candidates$2$co.parts) === null || _data$candidates$2$co2 === void 0 ? void 0 : _data$candidates$2$co2.length) > 0) {\n        // Safety check\n        if (data.candidates[0].finishReason === 'SAFETY') {\n          var _data$promptFeedback3;\n          console.warn('Gemini response blocked due to safety settings.');\n          if ((_data$promptFeedback3 = data.promptFeedback) !== null && _data$promptFeedback3 !== void 0 && _data$promptFeedback3.blockReason) {\n            console.warn(`Safety block reason: ${data.promptFeedback.blockReason}`);\n          }\n          // Add a placeholder to history to maintain turn structure\n          this.history.push({\n            role: 'model',\n            parts: [{\n              text: \"[Response blocked due to safety settings]\"\n            }]\n          });\n          return \"[Response blocked due to safety settings]\";\n        }\n        const aiResponseText = data.candidates[0].content.parts[0].text;\n\n        // Add AI response to history\n        this.history.push({\n          role: 'model',\n          parts: [{\n            text: aiResponseText\n          }]\n        });\n        return aiResponseText;\n      } else if ((_data$promptFeedback4 = data.promptFeedback) !== null && _data$promptFeedback4 !== void 0 && _data$promptFeedback4.blockReason) {\n        // Handle cases where the prompt itself was blocked\n        console.warn(`Gemini prompt blocked due to: ${data.promptFeedback.blockReason}`);\n        // Add a placeholder to history\n        this.history.push({\n          role: 'model',\n          // Still the model's turn, even if it's just an error message\n          parts: [{\n            text: `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]`\n          }]\n        });\n        return `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]`;\n      } else {\n        console.error('Unexpected roleplay API response structure:', data);\n        // Don't add a potentially invalid response to history\n        throw new Error('Invalid response format from Gemini API in roleplay');\n      }\n    } catch (error) {\n      console.error('Error calling Gemini API (roleplay):', error);\n      // Do not add user message to history if the API call failed before sending\n      // However, it was already added at the start of the try block.\n      // Consider adding a mechanism to remove the last user message if the API call fails critically.\n      // For now, just re-throw.\n      throw error;\n    }\n  }\n\n  /**\n   * Create the detailed system context from character and scenario\n   * @private\n   * @returns {string} - Formatted system context\n   */\n  _createSystemContext() {\n    // This function remains the same, defining the initial context.\n    const {\n      name,\n      age,\n      physicalDescription,\n      background,\n      personality\n    } = this.characterProfile || {};\n    const {\n      title,\n      setting,\n      initialSituation,\n      otherCharacters,\n      toneAndThemes\n    } = this.scenarioDetails || {};\n\n    // Create detailed system prompt - ensure defaults if objects are null/undefined\n    return `\nYou are an AI assisting in a roleplaying scenario titled \"${title || 'Untitled Scenario'}\". Your primary function is to portray all characters and narrative elements EXCEPT the main protagonist, whose inputs will be provided by the user.\n\n## Protagonist Character (User's Character)\nName: ${name || 'Unnamed'}\nAge: ${age || 'Unknown'}\nPhysical Description: ${physicalDescription || 'Not provided'}\nBackground: ${background || 'Not provided'}\nPersonality: ${personality || 'Not provided'}\n\n## Scenario Setting\nLocation: ${(setting === null || setting === void 0 ? void 0 : setting.location) || 'Unspecified'}\nTime: ${(setting === null || setting === void 0 ? void 0 : setting.time) || 'Unspecified'}\nAtmosphere: ${(setting === null || setting === void 0 ? void 0 : setting.atmosphere) || 'Unspecified'}\n\n## Initial Situation\n${initialSituation || 'No initial situation provided.'}\n\n## Other Characters (Portrayed by You, the AI)\n${(otherCharacters === null || otherCharacters === void 0 ? void 0 : otherCharacters.map(char => `\n- ${char.name} (${char.role || 'NPC'}): ${char.description || 'No description.'}\n  Relationship to protagonist: ${char.relationship || 'Not specified.'}\n`).join('')) || 'No other characters specified for you to portray.'}\n\n## Tone and Themes\n${toneAndThemes || 'No specific tone or themes specified.'}\n\n## Core Roleplay Instructions for You (the AI):\n1.  **Portray Non-Protagonist Roles:** You embody all characters listed under \"Other Characters\" and act as the narrator describing the environment and events. Do NOT act as the protagonist ('${name || 'Unnamed'}').\n2.  **Stay In Character:** Maintain the personalities and motivations of the characters you portray. Use descriptive language for narration.\n3.  **React Realistically:** Respond dynamically to the protagonist's dialogue, actions, and thoughts (marked as [DIALOGUE], [ACTION], [THOUGHT]).\n4.  **Maintain Atmosphere:** Ensure your responses reflect the specified tone, themes, and setting atmosphere.\n5.  **Format Clearly:** Use standard prose. Indicate dialogue using quotation marks (\" \"). Describe actions and scenery narratively. Avoid meta-commentary unless specifically instructed via a System Note.\n6.  **Advance the Story:** Collaborate with the user to move the narrative forward based on their choices, while staying true to the scenario.\n7.  **Response Length:** Aim for immersive yet reasonably concise responses (typically 1-4 paragraphs), unless the situation calls for more or less detail.\n8.  **Authenticity:** Be creative, emotionally resonant, and consistent with the established world and characters.\n9.  **No Meta-Roleplay:** Do not mention that this is a roleplay, that you are an AI, or discuss the instructions themselves within your narrative responses.\n`;\n  }\n\n  /**\n   * Save the current chat history\n   * @returns {Array<Object>} - The current chat history (array of user/model message objects)\n   */\n  exportChatHistory() {\n    // Make a deep copy to prevent external modifications affecting the service's state\n    return JSON.parse(JSON.stringify(this.history));\n  }\n\n  /**\n   * Import an existing chat history, replacing the current one.\n   * Validates the history format.\n   * @param {Array<Object>} history - Previous chat history (must be an array of {role: 'user'|'model', parts: [{text: string}]})\n   */\n  importChatHistory(history) {\n    // Basic validation\n    if (!Array.isArray(history) || history.some(msg => {\n      var _msg$parts$;\n      return !msg || typeof msg.role !== 'string' || !['user', 'model'].includes(msg.role) || !Array.isArray(msg.parts) || !((_msg$parts$ = msg.parts[0]) !== null && _msg$parts$ !== void 0 && _msg$parts$.text);\n    })) {\n      console.error(\"Invalid history format provided to importChatHistory. History not imported.\");\n      throw new Error(\"Invalid history format. Expected array of {role: 'user'|'model', parts: [{text: string}]}.\");\n    }\n    // Make a deep copy on import as well\n    this.history = JSON.parse(JSON.stringify(history));\n    console.log(`Imported chat history with ${this.history.length} messages.`);\n  }\n\n  /**\n   * Clears the current chat history and resets context (keeps API key and model).\n   * Re-initializes with existing character/scenario if available.\n   */\n  reset() {\n    console.log(\"Resetting GeminiService chat history and context...\");\n    const currentCharacterProfile = this.characterProfile;\n    const currentScenarioDetails = this.scenarioDetails;\n    this.history = [];\n    this.characterProfile = null;\n    this.scenarioDetails = null;\n    if (currentCharacterProfile && currentScenarioDetails) {\n      console.log(\"Re-initializing with previous character and scenario.\");\n      this.initialize(currentCharacterProfile, currentScenarioDetails);\n    } else {\n      console.log(\"Service reset. Call initialize() to set up character and scenario again.\");\n    }\n  }\n}\nexport default GeminiService;","map":{"version":3,"names":["GeminiService","constructor","apiKey","modelName","Error","baseUrl","history","characterProfile","scenarioDetails","console","log","initialize","systemContext","_createSystemContext","push","role","parts","text","roleplayInstructions","initialModelMessage","updateInstructions","instructions","instructionUpdateText","generateGeneric","prompt","_data$candidates$","_data$candidates$$con","_data$candidates$$con2","_data$promptFeedback2","contents","retries","delay","success","data","response","fetch","method","headers","body","JSON","stringify","generationConfig","temperature","topK","topP","maxOutputTokens","safetySettings","category","threshold","ok","errorText","error","status","json","err","Promise","resolve","setTimeout","candidates","content","length","finishReason","_data$promptFeedback","warn","promptFeedback","blockReason","generateResponse","userMessage","messageType","_data$candidates$2","_data$candidates$2$co","_data$candidates$2$co2","_data$promptFeedback4","toUpperCase","_data$promptFeedback3","aiResponseText","name","age","physicalDescription","background","personality","title","setting","initialSituation","otherCharacters","toneAndThemes","location","time","atmosphere","map","char","description","relationship","join","exportChatHistory","parse","importChatHistory","Array","isArray","some","msg","_msg$parts$","includes","reset","currentCharacterProfile","currentScenarioDetails"],"sources":["/Users/ryanmorrison/Code/narrativeforge/src/services/geminiService.js"],"sourcesContent":["/**\n * Service for handling interactions with Google's Gemini AI API\n */\nclass GeminiService {\n  /**\n   * Creates an instance of GeminiService.\n   * @param {string} apiKey - Your Google AI API key.\n   * @param {string} [modelName='gemini-2.5-flash-preview-04-17'] - The Gemini model to use.\n   *        Examples: 'gemini-1.5-pro-latest', 'gemini-1.5-flash-latest', 'gemini-pro'.\n   */\n  constructor(apiKey, modelName = 'gemini-2.0-flash') { // Default to the latest Pro model\n    if (!apiKey) {\n      throw new Error(\"API key is required for GeminiService.\");\n    }\n    this.apiKey = apiKey;\n    this.modelName = modelName;\n    // Construct the base URL using the provided model name\n    this.baseUrl = `https://generativelanguage.googleapis.com/v1beta/models/${this.modelName}:generateContent`;\n    this.history = [];\n    this.characterProfile = null;\n    this.scenarioDetails = null;\n\n    console.log(`GeminiService initialized with model: ${this.modelName}`);\n  }\n\n  /**\n   * Initialize the service with character and scenario information\n   * @param {Object} characterProfile - The detailed character profile\n   * @param {Object} scenarioDetails - The scenario setup details\n   */\n  initialize(characterProfile, scenarioDetails) {\n    this.characterProfile = characterProfile;\n    this.scenarioDetails = scenarioDetails;\n    this.history = [];\n\n    // Create system message that sets up the context\n    const systemContext = this._createSystemContext();\n    // System instructions should generally be 'user' role for the *first* turn\n    // Then the model responds. However, Gemini's multi-turn chat expects alternating roles.\n    // For initial setup, placing it as 'model' might work if followed by user input,\n    // but often it's better to combine setup instructions into the first 'user' message\n    // or use the dedicated `system_instruction` field if the API version supports it.\n    // Let's keep your structure for now, but be mindful of this.\n    this.history.push({\n      role: 'model', // Or potentially 'user' if it's the very first thing\n      parts: [{ text: systemContext }]\n    });\n\n    // If there are roleplay instructions, add them as system context (as 'model' role again)\n    if (scenarioDetails.roleplayInstructions) {\n       // Combine instructions with initial context or place strategically.\n       // Adding multiple 'model' turns consecutively might confuse the structure.\n       // Let's merge it into the first system message or ensure a 'user' turn happens before the actual chat.\n       // For simplicity, let's append it to the initial context message:\n       const initialModelMessage = this.history[0];\n       initialModelMessage.parts[0].text += `\\n\\nRoleplay Instructions:\\n\\n${scenarioDetails.roleplayInstructions}`;\n\n      // Original logic (kept commented out for reference):\n      // this.history.push({\n      //   role: 'model',\n      //   parts: [{ text: `Roleplay Instructions:\\n\\n${scenarioDetails.roleplayInstructions}` }]\n      // });\n    }\n     // Add an initial user prompt to kick off the conversation after setup\n     // This helps establish the user/model turn structure\n     this.history.push({\n        role: 'user',\n        parts: [{ text: \"Okay, I understand the setup. Let's begin the roleplay.\" }]\n     });\n     // Expect the model's first response acknowledging the start\n  }\n\n  /**\n   * Update roleplay instructions\n   * @param {string} instructions - New roleplay instructions\n   */\n  updateInstructions(instructions) {\n    // Find the *last* 'model' message that contains instructions, or add a new one.\n    // It's often better to send updates as a 'user' message instructing the model.\n    // Let's try sending it as a user directive.\n\n    const instructionUpdateText = `(System Note: Please adhere to the following updated roleplay instructions from now on, incorporating them naturally into your responses without explicitly mentioning this note: ${instructions})`;\n\n    this.history.push({\n      role: 'user', // Send as a user message to instruct the model for the next turn\n      parts: [{ text: instructionUpdateText }]\n    });\n\n    // The model's *next* response should implicitly follow these instructions.\n    // Avoid adding extra 'model' reminders as it pollutes the history.\n\n    // --- Original Logic (commented out) ---\n    // // Find any existing instruction message\n    // const existingInstructionIndex = this.history.findIndex(msg =>\n    //   msg.role === 'model' &&\n    //   msg.parts[0]?.text?.includes('Roleplay Instructions:')\n    // );\n    //\n    // // Create the instruction message\n    // const instructionMessage = {\n    //   role: 'model', // This can disrupt the turn flow if inserted arbitrarily\n    //   parts: [{ text: `Roleplay Instructions:\\n\\n${instructions}` }]\n    // };\n    //\n    // if (existingInstructionIndex !== -1) {\n    //   // Replace existing instructions\n    //   this.history[existingInstructionIndex] = instructionMessage;\n    // } else {\n    //   // Add new instructions after the system context\n    //   // Finding the right place to insert can be tricky. Appending might be safer.\n    //   this.history.splice(1, 0, instructionMessage); // Inserting might break user/model alternation\n    // }\n    //\n    // // Add a reminder to follow the new instructions (not visible to user) - Avoid this if possible\n    // this.history.push({\n    //   role: 'model',\n    //   parts: [{ text: 'Remember to follow the updated roleplay instructions provided above, but do not explicitly mention them in your responses.' }]\n    // });\n    // --- End Original Logic ---\n  }\n\n\n  /**\n   * Generate a generic response from Gemini based on a prompt (uses minimal history)\n   * @param {string} prompt - The prompt to send to Gemini\n   * @returns {Promise<string>} - The AI generated response\n   */\n  async generateGeneric(prompt) {\n    // This function seems intended for non-chat, single-turn generation.\n    // It should NOT use the main chat history.\n    try {\n      console.log(`Sending generic request to Gemini API (${this.modelName})`);\n      console.log('Using API URL:', this.baseUrl);\n\n      // Prepare contents for a single-turn request\n      const contents = [{\n        role: 'user',\n        parts: [{ text: prompt }]\n      }];\n\n      // Add retry logic with exponential backoff (same as before)\n      let retries = 3;\n      let delay = 1000; // Start with 1 second delay\n      let success = false;\n      let data;\n\n      while (retries > 0 && !success) {\n        try {\n          const response = await fetch(`${this.baseUrl}?key=${this.apiKey}`, {\n            method: 'POST',\n            headers: {\n              'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({\n              contents: contents, // Use the single prompt content\n              generationConfig: {\n                temperature: 0.7,\n                topK: 40,\n                topP: 0.95,\n                maxOutputTokens: 1000, // Adjust as needed\n              },\n              // Safety settings might also be relevant here if needed\n               safetySettings: [ // Keep safety settings consistent\n                 { category: \"HARM_CATEGORY_HARASSMENT\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" },\n                 { category: \"HARM_CATEGORY_HATE_SPEECH\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" },\n                 { category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold: \"BLOCK_ONLY_HIGH\" },\n                 { category: \"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" }\n               ]\n            })\n          });\n\n          // Check HTTP status (same as before)\n          if (!response.ok) {\n            const errorText = await response.text();\n            console.error('API error response (generic):', errorText);\n            // Throw specific errors (same as before)\n             if (response.status === 400) throw new Error(`API error: 400 - Bad Request. Check model name ('${this.modelName}') and parameters. Details: ${errorText}`);\n             if (response.status === 401) throw new Error(`API error: 401 - Unauthorized. Your API key may be invalid or expired.`);\n             if (response.status === 403) throw new Error(`API error: 403 - Forbidden. Your API key may not have access to '${this.modelName}' or you've exceeded quota.`);\n             if (response.status === 404) throw new Error(`API error: 404 - Not Found. The specified model '${this.modelName}' may not exist or the URL is incorrect.`);\n             if (response.status === 429) throw new Error(`API error: 429 - Too Many Requests. You've hit rate limits or quota restrictions.`);\n             throw new Error(`API error (generic): ${response.status} - ${errorText || 'Unknown error'}`);\n          }\n\n          data = await response.json();\n          // console.log('API response structure (generic):', JSON.stringify(data, null, 2).substring(0, 500) + '...'); // Optional: for debugging\n          success = true;\n        } catch (err) {\n          console.error(`Attempt ${4 - retries} failed (generic):`, err);\n          retries--;\n          if (retries > 0) {\n            console.log(`Retrying in ${delay}ms...`);\n            await new Promise(resolve => setTimeout(resolve, delay));\n            delay *= 2; // Exponential backoff\n          } else {\n            throw err; // Re-throw the last error if all retries failed\n          }\n        }\n      }\n\n      // Extract the model's response (same as before)\n      if (data.candidates && data.candidates[0]?.content?.parts?.length > 0) {\n        // Safety check: Check for finishReason\n        if (data.candidates[0].finishReason === 'SAFETY') {\n            console.warn('Gemini response blocked due to safety settings.');\n            // Optionally check promptFeedback for block reason\n            if (data.promptFeedback?.blockReason) {\n                 console.warn(`Safety block reason: ${data.promptFeedback.blockReason}`);\n            }\n            return \"[Response blocked due to safety settings]\";\n        }\n        return data.candidates[0].content.parts[0].text;\n      } else if (data.promptFeedback?.blockReason) {\n          // Handle cases where the prompt itself was blocked\n          console.warn(`Gemini prompt blocked due to: ${data.promptFeedback.blockReason}`);\n          return `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]`;\n      }\n       else {\n        console.error('Unexpected API response structure (generic):', data);\n        throw new Error('Invalid response format from Gemini API (generic)');\n      }\n    } catch (error) {\n      console.error('Error calling Gemini API (generic):', error);\n      // Don't add generic prompt/response to history\n      throw error;\n    }\n  }\n\n\n  /**\n   * Generate a response from Gemini based on user input and chat history\n   * @param {string} userMessage - The user's message content\n   * @param {string} messageType - Type of message (dialogue, action, thought)\n   * @returns {Promise<string>} - The AI generated response\n   */\n  async generateResponse(userMessage, messageType) {\n    try {\n      // Add user message to history\n      this.history.push({\n        role: 'user',\n        parts: [{ text: `[${messageType.toUpperCase()}] ${userMessage}` }]\n      });\n\n      // Ensure history alternates user/model roles correctly.\n      // If the last two roles are the same, it might indicate an issue.\n      // Basic check:\n      if (this.history.length >= 2 && this.history[this.history.length - 1].role === this.history[this.history.length - 2].role) {\n          console.warn(\"Potential history role alternation issue detected before API call.\");\n          // Depending on the cause, you might need to adjust `initialize` or `updateInstructions`\n      }\n\n      // Prepare the context window for Gemini (using the full history)\n      const contents = [...this.history];\n\n      console.log(`Sending roleplay request to Gemini API (${this.modelName})`);\n      // console.log('Using API URL:', this.baseUrl); // Already logged in constructor/generic\n      // console.log('Sending history:', JSON.stringify(contents, null, 2)); // Debugging: log history being sent\n\n      // Add retry logic with exponential backoff (same as before)\n      let retries = 3;\n      let delay = 1000;\n      let success = false;\n      let data;\n\n      while (retries > 0 && !success) {\n        try {\n          const response = await fetch(`${this.baseUrl}?key=${this.apiKey}`, {\n            method: 'POST',\n            headers: {\n              'Content-Type': 'application/json',\n            },\n            body: JSON.stringify({\n              contents: contents, // Send the chat history\n              generationConfig: {\n                temperature: 0.7, // Adjust as needed for creativity vs coherence\n                topK: 40,\n                topP: 0.95,\n                maxOutputTokens: 800, // Adjust based on desired response length\n                // stopSequences: [] // Optional: sequences that stop generation\n              },\n              safetySettings: [ // Keep safety settings consistent\n                { category: \"HARM_CATEGORY_HARASSMENT\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" },\n                { category: \"HARM_CATEGORY_HATE_SPEECH\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" },\n                { category: \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold: \"BLOCK_ONLY_HIGH\" }, // Be cautious with roleplay\n                { category: \"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold: \"BLOCK_MEDIUM_AND_ABOVE\" }\n              ]\n            })\n          });\n\n          // Check HTTP status (same as before)\n          if (!response.ok) {\n             const errorText = await response.text();\n             console.error('API error response (roleplay):', errorText);\n             // Throw specific errors (same as before)\n             if (response.status === 400) throw new Error(`API error: 400 - Bad Request. Check model name ('${this.modelName}'), history structure, or parameters. Details: ${errorText}`);\n             if (response.status === 401) throw new Error(`API error: 401 - Unauthorized. Your API key may be invalid or expired.`);\n             if (response.status === 403) throw new Error(`API error: 403 - Forbidden. Your API key may not have access to '${this.modelName}' or you've exceeded quota.`);\n             if (response.status === 404) throw new Error(`API error: 404 - Not Found. The specified model '${this.modelName}' may not exist or the URL is incorrect.`);\n             if (response.status === 429) throw new Error(`API error: 429 - Too Many Requests. You've hit rate limits or quota restrictions.`);\n             throw new Error(`API error (roleplay): ${response.status} - ${errorText || 'Unknown error'}`);\n          }\n\n          data = await response.json();\n          success = true;\n\n        } catch (err) {\n          console.error(`Attempt ${4 - retries} failed (roleplay):`, err);\n          retries--;\n          if (retries > 0) {\n            console.log(`Retrying in ${delay}ms...`);\n            await new Promise(resolve => setTimeout(resolve, delay));\n            delay *= 2;\n          } else {\n            throw err;\n          }\n        }\n      }\n\n      // Extract the model's response\n        if (data.candidates && data.candidates[0]?.content?.parts?.length > 0) {\n            // Safety check\n            if (data.candidates[0].finishReason === 'SAFETY') {\n                console.warn('Gemini response blocked due to safety settings.');\n                 if (data.promptFeedback?.blockReason) {\n                     console.warn(`Safety block reason: ${data.promptFeedback.blockReason}`);\n                 }\n                // Add a placeholder to history to maintain turn structure\n                this.history.push({\n                    role: 'model',\n                    parts: [{ text: \"[Response blocked due to safety settings]\" }]\n                });\n                return \"[Response blocked due to safety settings]\";\n            }\n\n            const aiResponseText = data.candidates[0].content.parts[0].text;\n\n            // Add AI response to history\n            this.history.push({\n                role: 'model',\n                parts: [{ text: aiResponseText }]\n            });\n\n            return aiResponseText;\n        } else if (data.promptFeedback?.blockReason) {\n             // Handle cases where the prompt itself was blocked\n             console.warn(`Gemini prompt blocked due to: ${data.promptFeedback.blockReason}`);\n             // Add a placeholder to history\n             this.history.push({\n                 role: 'model', // Still the model's turn, even if it's just an error message\n                 parts: [{ text: `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]` }]\n             });\n             return `[Prompt blocked due to safety settings: ${data.promptFeedback.blockReason}]`;\n        }\n        else {\n            console.error('Unexpected roleplay API response structure:', data);\n            // Don't add a potentially invalid response to history\n            throw new Error('Invalid response format from Gemini API in roleplay');\n        }\n    } catch (error) {\n      console.error('Error calling Gemini API (roleplay):', error);\n      // Do not add user message to history if the API call failed before sending\n      // However, it was already added at the start of the try block.\n      // Consider adding a mechanism to remove the last user message if the API call fails critically.\n      // For now, just re-throw.\n      throw error;\n    }\n  }\n\n  /**\n   * Create the detailed system context from character and scenario\n   * @private\n   * @returns {string} - Formatted system context\n   */\n  _createSystemContext() {\n    // This function remains the same, defining the initial context.\n    const { name, age, physicalDescription, background, personality } = this.characterProfile || {};\n    const { title, setting, initialSituation, otherCharacters, toneAndThemes } = this.scenarioDetails || {};\n\n    // Create detailed system prompt - ensure defaults if objects are null/undefined\n    return `\nYou are an AI assisting in a roleplaying scenario titled \"${title || 'Untitled Scenario'}\". Your primary function is to portray all characters and narrative elements EXCEPT the main protagonist, whose inputs will be provided by the user.\n\n## Protagonist Character (User's Character)\nName: ${name || 'Unnamed'}\nAge: ${age || 'Unknown'}\nPhysical Description: ${physicalDescription || 'Not provided'}\nBackground: ${background || 'Not provided'}\nPersonality: ${personality || 'Not provided'}\n\n## Scenario Setting\nLocation: ${setting?.location || 'Unspecified'}\nTime: ${setting?.time || 'Unspecified'}\nAtmosphere: ${setting?.atmosphere || 'Unspecified'}\n\n## Initial Situation\n${initialSituation || 'No initial situation provided.'}\n\n## Other Characters (Portrayed by You, the AI)\n${otherCharacters?.map(char => `\n- ${char.name} (${char.role || 'NPC'}): ${char.description || 'No description.'}\n  Relationship to protagonist: ${char.relationship || 'Not specified.'}\n`).join('') || 'No other characters specified for you to portray.'}\n\n## Tone and Themes\n${toneAndThemes || 'No specific tone or themes specified.'}\n\n## Core Roleplay Instructions for You (the AI):\n1.  **Portray Non-Protagonist Roles:** You embody all characters listed under \"Other Characters\" and act as the narrator describing the environment and events. Do NOT act as the protagonist ('${name || 'Unnamed'}').\n2.  **Stay In Character:** Maintain the personalities and motivations of the characters you portray. Use descriptive language for narration.\n3.  **React Realistically:** Respond dynamically to the protagonist's dialogue, actions, and thoughts (marked as [DIALOGUE], [ACTION], [THOUGHT]).\n4.  **Maintain Atmosphere:** Ensure your responses reflect the specified tone, themes, and setting atmosphere.\n5.  **Format Clearly:** Use standard prose. Indicate dialogue using quotation marks (\" \"). Describe actions and scenery narratively. Avoid meta-commentary unless specifically instructed via a System Note.\n6.  **Advance the Story:** Collaborate with the user to move the narrative forward based on their choices, while staying true to the scenario.\n7.  **Response Length:** Aim for immersive yet reasonably concise responses (typically 1-4 paragraphs), unless the situation calls for more or less detail.\n8.  **Authenticity:** Be creative, emotionally resonant, and consistent with the established world and characters.\n9.  **No Meta-Roleplay:** Do not mention that this is a roleplay, that you are an AI, or discuss the instructions themselves within your narrative responses.\n`;\n  }\n\n  /**\n   * Save the current chat history\n   * @returns {Array<Object>} - The current chat history (array of user/model message objects)\n   */\n  exportChatHistory() {\n    // Make a deep copy to prevent external modifications affecting the service's state\n    return JSON.parse(JSON.stringify(this.history));\n  }\n\n  /**\n   * Import an existing chat history, replacing the current one.\n   * Validates the history format.\n   * @param {Array<Object>} history - Previous chat history (must be an array of {role: 'user'|'model', parts: [{text: string}]})\n   */\n  importChatHistory(history) {\n     // Basic validation\n    if (!Array.isArray(history) || history.some(msg => !msg || typeof msg.role !== 'string' || !['user', 'model'].includes(msg.role) || !Array.isArray(msg.parts) || !msg.parts[0]?.text)) {\n        console.error(\"Invalid history format provided to importChatHistory. History not imported.\");\n        throw new Error(\"Invalid history format. Expected array of {role: 'user'|'model', parts: [{text: string}]}.\");\n    }\n    // Make a deep copy on import as well\n    this.history = JSON.parse(JSON.stringify(history));\n    console.log(`Imported chat history with ${this.history.length} messages.`);\n  }\n\n  /**\n   * Clears the current chat history and resets context (keeps API key and model).\n   * Re-initializes with existing character/scenario if available.\n   */\n  reset() {\n    console.log(\"Resetting GeminiService chat history and context...\");\n    const currentCharacterProfile = this.characterProfile;\n    const currentScenarioDetails = this.scenarioDetails;\n    this.history = [];\n    this.characterProfile = null;\n    this.scenarioDetails = null;\n\n    if (currentCharacterProfile && currentScenarioDetails) {\n        console.log(\"Re-initializing with previous character and scenario.\");\n        this.initialize(currentCharacterProfile, currentScenarioDetails);\n    } else {\n        console.log(\"Service reset. Call initialize() to set up character and scenario again.\");\n    }\n  }\n}\n\nexport default GeminiService;"],"mappings":"AAAA;AACA;AACA;AACA,MAAMA,aAAa,CAAC;EAClB;AACF;AACA;AACA;AACA;AACA;EACEC,WAAWA,CAACC,MAAM,EAAEC,SAAS,GAAG,kBAAkB,EAAE;IAAE;IACpD,IAAI,CAACD,MAAM,EAAE;MACX,MAAM,IAAIE,KAAK,CAAC,wCAAwC,CAAC;IAC3D;IACA,IAAI,CAACF,MAAM,GAAGA,MAAM;IACpB,IAAI,CAACC,SAAS,GAAGA,SAAS;IAC1B;IACA,IAAI,CAACE,OAAO,GAAG,2DAA2D,IAAI,CAACF,SAAS,kBAAkB;IAC1G,IAAI,CAACG,OAAO,GAAG,EAAE;IACjB,IAAI,CAACC,gBAAgB,GAAG,IAAI;IAC5B,IAAI,CAACC,eAAe,GAAG,IAAI;IAE3BC,OAAO,CAACC,GAAG,CAAC,yCAAyC,IAAI,CAACP,SAAS,EAAE,CAAC;EACxE;;EAEA;AACF;AACA;AACA;AACA;EACEQ,UAAUA,CAACJ,gBAAgB,EAAEC,eAAe,EAAE;IAC5C,IAAI,CAACD,gBAAgB,GAAGA,gBAAgB;IACxC,IAAI,CAACC,eAAe,GAAGA,eAAe;IACtC,IAAI,CAACF,OAAO,GAAG,EAAE;;IAEjB;IACA,MAAMM,aAAa,GAAG,IAAI,CAACC,oBAAoB,CAAC,CAAC;IACjD;IACA;IACA;IACA;IACA;IACA;IACA,IAAI,CAACP,OAAO,CAACQ,IAAI,CAAC;MAChBC,IAAI,EAAE,OAAO;MAAE;MACfC,KAAK,EAAE,CAAC;QAAEC,IAAI,EAAEL;MAAc,CAAC;IACjC,CAAC,CAAC;;IAEF;IACA,IAAIJ,eAAe,CAACU,oBAAoB,EAAE;MACvC;MACA;MACA;MACA;MACA,MAAMC,mBAAmB,GAAG,IAAI,CAACb,OAAO,CAAC,CAAC,CAAC;MAC3Ca,mBAAmB,CAACH,KAAK,CAAC,CAAC,CAAC,CAACC,IAAI,IAAI,iCAAiCT,eAAe,CAACU,oBAAoB,EAAE;;MAE7G;MACA;MACA;MACA;MACA;IACF;IACC;IACA;IACA,IAAI,CAACZ,OAAO,CAACQ,IAAI,CAAC;MACfC,IAAI,EAAE,MAAM;MACZC,KAAK,EAAE,CAAC;QAAEC,IAAI,EAAE;MAA0D,CAAC;IAC9E,CAAC,CAAC;IACF;EACH;;EAEA;AACF;AACA;AACA;EACEG,kBAAkBA,CAACC,YAAY,EAAE;IAC/B;IACA;IACA;;IAEA,MAAMC,qBAAqB,GAAG,qLAAqLD,YAAY,GAAG;IAElO,IAAI,CAACf,OAAO,CAACQ,IAAI,CAAC;MAChBC,IAAI,EAAE,MAAM;MAAE;MACdC,KAAK,EAAE,CAAC;QAAEC,IAAI,EAAEK;MAAsB,CAAC;IACzC,CAAC,CAAC;;IAEF;IACA;;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;EACF;;EAGA;AACF;AACA;AACA;AACA;EACE,MAAMC,eAAeA,CAACC,MAAM,EAAE;IAC5B;IACA;IACA,IAAI;MAAA,IAAAC,iBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,qBAAA;MACFnB,OAAO,CAACC,GAAG,CAAC,0CAA0C,IAAI,CAACP,SAAS,GAAG,CAAC;MACxEM,OAAO,CAACC,GAAG,CAAC,gBAAgB,EAAE,IAAI,CAACL,OAAO,CAAC;;MAE3C;MACA,MAAMwB,QAAQ,GAAG,CAAC;QAChBd,IAAI,EAAE,MAAM;QACZC,KAAK,EAAE,CAAC;UAAEC,IAAI,EAAEO;QAAO,CAAC;MAC1B,CAAC,CAAC;;MAEF;MACA,IAAIM,OAAO,GAAG,CAAC;MACf,IAAIC,KAAK,GAAG,IAAI,CAAC,CAAC;MAClB,IAAIC,OAAO,GAAG,KAAK;MACnB,IAAIC,IAAI;MAER,OAAOH,OAAO,GAAG,CAAC,IAAI,CAACE,OAAO,EAAE;QAC9B,IAAI;UACF,MAAME,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAC9B,OAAO,QAAQ,IAAI,CAACH,MAAM,EAAE,EAAE;YACjEkC,MAAM,EAAE,MAAM;YACdC,OAAO,EAAE;cACP,cAAc,EAAE;YAClB,CAAC;YACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;cACnBX,QAAQ,EAAEA,QAAQ;cAAE;cACpBY,gBAAgB,EAAE;gBAChBC,WAAW,EAAE,GAAG;gBAChBC,IAAI,EAAE,EAAE;gBACRC,IAAI,EAAE,IAAI;gBACVC,eAAe,EAAE,IAAI,CAAE;cACzB,CAAC;cACD;cACCC,cAAc,EAAE;cAAE;cAChB;gBAAEC,QAAQ,EAAE,0BAA0B;gBAAEC,SAAS,EAAE;cAAyB,CAAC,EAC7E;gBAAED,QAAQ,EAAE,2BAA2B;gBAAEC,SAAS,EAAE;cAAyB,CAAC,EAC9E;gBAAED,QAAQ,EAAE,iCAAiC;gBAAEC,SAAS,EAAE;cAAkB,CAAC,EAC7E;gBAAED,QAAQ,EAAE,iCAAiC;gBAAEC,SAAS,EAAE;cAAyB,CAAC;YAEzF,CAAC;UACH,CAAC,CAAC;;UAEF;UACA,IAAI,CAACd,QAAQ,CAACe,EAAE,EAAE;YAChB,MAAMC,SAAS,GAAG,MAAMhB,QAAQ,CAACjB,IAAI,CAAC,CAAC;YACvCR,OAAO,CAAC0C,KAAK,CAAC,+BAA+B,EAAED,SAAS,CAAC;YACzD;YACC,IAAIhB,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oDAAoD,IAAI,CAACD,SAAS,+BAA+B+C,SAAS,EAAE,CAAC;YAC1J,IAAIhB,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,wEAAwE,CAAC;YACtH,IAAI8B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oEAAoE,IAAI,CAACD,SAAS,6BAA6B,CAAC;YAC7J,IAAI+B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oDAAoD,IAAI,CAACD,SAAS,0CAA0C,CAAC;YAC1J,IAAI+B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,mFAAmF,CAAC;YACjI,MAAM,IAAIA,KAAK,CAAC,wBAAwB8B,QAAQ,CAACkB,MAAM,MAAMF,SAAS,IAAI,eAAe,EAAE,CAAC;UAC/F;UAEAjB,IAAI,GAAG,MAAMC,QAAQ,CAACmB,IAAI,CAAC,CAAC;UAC5B;UACArB,OAAO,GAAG,IAAI;QAChB,CAAC,CAAC,OAAOsB,GAAG,EAAE;UACZ7C,OAAO,CAAC0C,KAAK,CAAC,WAAW,CAAC,GAAGrB,OAAO,oBAAoB,EAAEwB,GAAG,CAAC;UAC9DxB,OAAO,EAAE;UACT,IAAIA,OAAO,GAAG,CAAC,EAAE;YACfrB,OAAO,CAACC,GAAG,CAAC,eAAeqB,KAAK,OAAO,CAAC;YACxC,MAAM,IAAIwB,OAAO,CAACC,OAAO,IAAIC,UAAU,CAACD,OAAO,EAAEzB,KAAK,CAAC,CAAC;YACxDA,KAAK,IAAI,CAAC,CAAC,CAAC;UACd,CAAC,MAAM;YACL,MAAMuB,GAAG,CAAC,CAAC;UACb;QACF;MACF;;MAEA;MACA,IAAIrB,IAAI,CAACyB,UAAU,IAAI,EAAAjC,iBAAA,GAAAQ,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,cAAAjC,iBAAA,wBAAAC,qBAAA,GAAlBD,iBAAA,CAAoBkC,OAAO,cAAAjC,qBAAA,wBAAAC,sBAAA,GAA3BD,qBAAA,CAA6BV,KAAK,cAAAW,sBAAA,uBAAlCA,sBAAA,CAAoCiC,MAAM,IAAG,CAAC,EAAE;QACrE;QACA,IAAI3B,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,CAACG,YAAY,KAAK,QAAQ,EAAE;UAAA,IAAAC,oBAAA;UAC9CrD,OAAO,CAACsD,IAAI,CAAC,iDAAiD,CAAC;UAC/D;UACA,KAAAD,oBAAA,GAAI7B,IAAI,CAAC+B,cAAc,cAAAF,oBAAA,eAAnBA,oBAAA,CAAqBG,WAAW,EAAE;YACjCxD,OAAO,CAACsD,IAAI,CAAC,wBAAwB9B,IAAI,CAAC+B,cAAc,CAACC,WAAW,EAAE,CAAC;UAC5E;UACA,OAAO,2CAA2C;QACtD;QACA,OAAOhC,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC3C,KAAK,CAAC,CAAC,CAAC,CAACC,IAAI;MACjD,CAAC,MAAM,KAAAW,qBAAA,GAAIK,IAAI,CAAC+B,cAAc,cAAApC,qBAAA,eAAnBA,qBAAA,CAAqBqC,WAAW,EAAE;QACzC;QACAxD,OAAO,CAACsD,IAAI,CAAC,iCAAiC9B,IAAI,CAAC+B,cAAc,CAACC,WAAW,EAAE,CAAC;QAChF,OAAO,2CAA2ChC,IAAI,CAAC+B,cAAc,CAACC,WAAW,GAAG;MACxF,CAAC,MACK;QACJxD,OAAO,CAAC0C,KAAK,CAAC,8CAA8C,EAAElB,IAAI,CAAC;QACnE,MAAM,IAAI7B,KAAK,CAAC,mDAAmD,CAAC;MACtE;IACF,CAAC,CAAC,OAAO+C,KAAK,EAAE;MACd1C,OAAO,CAAC0C,KAAK,CAAC,qCAAqC,EAAEA,KAAK,CAAC;MAC3D;MACA,MAAMA,KAAK;IACb;EACF;;EAGA;AACF;AACA;AACA;AACA;AACA;EACE,MAAMe,gBAAgBA,CAACC,WAAW,EAAEC,WAAW,EAAE;IAC/C,IAAI;MAAA,IAAAC,kBAAA,EAAAC,qBAAA,EAAAC,sBAAA,EAAAC,qBAAA;MACF;MACA,IAAI,CAAClE,OAAO,CAACQ,IAAI,CAAC;QAChBC,IAAI,EAAE,MAAM;QACZC,KAAK,EAAE,CAAC;UAAEC,IAAI,EAAE,IAAImD,WAAW,CAACK,WAAW,CAAC,CAAC,KAAKN,WAAW;QAAG,CAAC;MACnE,CAAC,CAAC;;MAEF;MACA;MACA;MACA,IAAI,IAAI,CAAC7D,OAAO,CAACsD,MAAM,IAAI,CAAC,IAAI,IAAI,CAACtD,OAAO,CAAC,IAAI,CAACA,OAAO,CAACsD,MAAM,GAAG,CAAC,CAAC,CAAC7C,IAAI,KAAK,IAAI,CAACT,OAAO,CAAC,IAAI,CAACA,OAAO,CAACsD,MAAM,GAAG,CAAC,CAAC,CAAC7C,IAAI,EAAE;QACvHN,OAAO,CAACsD,IAAI,CAAC,oEAAoE,CAAC;QAClF;MACJ;;MAEA;MACA,MAAMlC,QAAQ,GAAG,CAAC,GAAG,IAAI,CAACvB,OAAO,CAAC;MAElCG,OAAO,CAACC,GAAG,CAAC,2CAA2C,IAAI,CAACP,SAAS,GAAG,CAAC;MACzE;MACA;;MAEA;MACA,IAAI2B,OAAO,GAAG,CAAC;MACf,IAAIC,KAAK,GAAG,IAAI;MAChB,IAAIC,OAAO,GAAG,KAAK;MACnB,IAAIC,IAAI;MAER,OAAOH,OAAO,GAAG,CAAC,IAAI,CAACE,OAAO,EAAE;QAC9B,IAAI;UACF,MAAME,QAAQ,GAAG,MAAMC,KAAK,CAAC,GAAG,IAAI,CAAC9B,OAAO,QAAQ,IAAI,CAACH,MAAM,EAAE,EAAE;YACjEkC,MAAM,EAAE,MAAM;YACdC,OAAO,EAAE;cACP,cAAc,EAAE;YAClB,CAAC;YACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;cACnBX,QAAQ,EAAEA,QAAQ;cAAE;cACpBY,gBAAgB,EAAE;gBAChBC,WAAW,EAAE,GAAG;gBAAE;gBAClBC,IAAI,EAAE,EAAE;gBACRC,IAAI,EAAE,IAAI;gBACVC,eAAe,EAAE,GAAG,CAAE;gBACtB;cACF,CAAC;cACDC,cAAc,EAAE;cAAE;cAChB;gBAAEC,QAAQ,EAAE,0BAA0B;gBAAEC,SAAS,EAAE;cAAyB,CAAC,EAC7E;gBAAED,QAAQ,EAAE,2BAA2B;gBAAEC,SAAS,EAAE;cAAyB,CAAC,EAC9E;gBAAED,QAAQ,EAAE,iCAAiC;gBAAEC,SAAS,EAAE;cAAkB,CAAC;cAAE;cAC/E;gBAAED,QAAQ,EAAE,iCAAiC;gBAAEC,SAAS,EAAE;cAAyB,CAAC;YAExF,CAAC;UACH,CAAC,CAAC;;UAEF;UACA,IAAI,CAACd,QAAQ,CAACe,EAAE,EAAE;YACf,MAAMC,SAAS,GAAG,MAAMhB,QAAQ,CAACjB,IAAI,CAAC,CAAC;YACvCR,OAAO,CAAC0C,KAAK,CAAC,gCAAgC,EAAED,SAAS,CAAC;YAC1D;YACA,IAAIhB,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oDAAoD,IAAI,CAACD,SAAS,kDAAkD+C,SAAS,EAAE,CAAC;YAC7K,IAAIhB,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,wEAAwE,CAAC;YACtH,IAAI8B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oEAAoE,IAAI,CAACD,SAAS,6BAA6B,CAAC;YAC7J,IAAI+B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,oDAAoD,IAAI,CAACD,SAAS,0CAA0C,CAAC;YAC1J,IAAI+B,QAAQ,CAACkB,MAAM,KAAK,GAAG,EAAE,MAAM,IAAIhD,KAAK,CAAC,mFAAmF,CAAC;YACjI,MAAM,IAAIA,KAAK,CAAC,yBAAyB8B,QAAQ,CAACkB,MAAM,MAAMF,SAAS,IAAI,eAAe,EAAE,CAAC;UAChG;UAEAjB,IAAI,GAAG,MAAMC,QAAQ,CAACmB,IAAI,CAAC,CAAC;UAC5BrB,OAAO,GAAG,IAAI;QAEhB,CAAC,CAAC,OAAOsB,GAAG,EAAE;UACZ7C,OAAO,CAAC0C,KAAK,CAAC,WAAW,CAAC,GAAGrB,OAAO,qBAAqB,EAAEwB,GAAG,CAAC;UAC/DxB,OAAO,EAAE;UACT,IAAIA,OAAO,GAAG,CAAC,EAAE;YACfrB,OAAO,CAACC,GAAG,CAAC,eAAeqB,KAAK,OAAO,CAAC;YACxC,MAAM,IAAIwB,OAAO,CAACC,OAAO,IAAIC,UAAU,CAACD,OAAO,EAAEzB,KAAK,CAAC,CAAC;YACxDA,KAAK,IAAI,CAAC;UACZ,CAAC,MAAM;YACL,MAAMuB,GAAG;UACX;QACF;MACF;;MAEA;MACE,IAAIrB,IAAI,CAACyB,UAAU,IAAI,EAAAW,kBAAA,GAAApC,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,cAAAW,kBAAA,wBAAAC,qBAAA,GAAlBD,kBAAA,CAAoBV,OAAO,cAAAW,qBAAA,wBAAAC,sBAAA,GAA3BD,qBAAA,CAA6BtD,KAAK,cAAAuD,sBAAA,uBAAlCA,sBAAA,CAAoCX,MAAM,IAAG,CAAC,EAAE;QACnE;QACA,IAAI3B,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,CAACG,YAAY,KAAK,QAAQ,EAAE;UAAA,IAAAa,qBAAA;UAC9CjE,OAAO,CAACsD,IAAI,CAAC,iDAAiD,CAAC;UAC9D,KAAAW,qBAAA,GAAIzC,IAAI,CAAC+B,cAAc,cAAAU,qBAAA,eAAnBA,qBAAA,CAAqBT,WAAW,EAAE;YAClCxD,OAAO,CAACsD,IAAI,CAAC,wBAAwB9B,IAAI,CAAC+B,cAAc,CAACC,WAAW,EAAE,CAAC;UAC3E;UACD;UACA,IAAI,CAAC3D,OAAO,CAACQ,IAAI,CAAC;YACdC,IAAI,EAAE,OAAO;YACbC,KAAK,EAAE,CAAC;cAAEC,IAAI,EAAE;YAA4C,CAAC;UACjE,CAAC,CAAC;UACF,OAAO,2CAA2C;QACtD;QAEA,MAAM0D,cAAc,GAAG1C,IAAI,CAACyB,UAAU,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC3C,KAAK,CAAC,CAAC,CAAC,CAACC,IAAI;;QAE/D;QACA,IAAI,CAACX,OAAO,CAACQ,IAAI,CAAC;UACdC,IAAI,EAAE,OAAO;UACbC,KAAK,EAAE,CAAC;YAAEC,IAAI,EAAE0D;UAAe,CAAC;QACpC,CAAC,CAAC;QAEF,OAAOA,cAAc;MACzB,CAAC,MAAM,KAAAH,qBAAA,GAAIvC,IAAI,CAAC+B,cAAc,cAAAQ,qBAAA,eAAnBA,qBAAA,CAAqBP,WAAW,EAAE;QACxC;QACAxD,OAAO,CAACsD,IAAI,CAAC,iCAAiC9B,IAAI,CAAC+B,cAAc,CAACC,WAAW,EAAE,CAAC;QAChF;QACA,IAAI,CAAC3D,OAAO,CAACQ,IAAI,CAAC;UACdC,IAAI,EAAE,OAAO;UAAE;UACfC,KAAK,EAAE,CAAC;YAAEC,IAAI,EAAE,2CAA2CgB,IAAI,CAAC+B,cAAc,CAACC,WAAW;UAAI,CAAC;QACnG,CAAC,CAAC;QACF,OAAO,2CAA2ChC,IAAI,CAAC+B,cAAc,CAACC,WAAW,GAAG;MACzF,CAAC,MACI;QACDxD,OAAO,CAAC0C,KAAK,CAAC,6CAA6C,EAAElB,IAAI,CAAC;QAClE;QACA,MAAM,IAAI7B,KAAK,CAAC,qDAAqD,CAAC;MAC1E;IACJ,CAAC,CAAC,OAAO+C,KAAK,EAAE;MACd1C,OAAO,CAAC0C,KAAK,CAAC,sCAAsC,EAAEA,KAAK,CAAC;MAC5D;MACA;MACA;MACA;MACA,MAAMA,KAAK;IACb;EACF;;EAEA;AACF;AACA;AACA;AACA;EACEtC,oBAAoBA,CAAA,EAAG;IACrB;IACA,MAAM;MAAE+D,IAAI;MAAEC,GAAG;MAAEC,mBAAmB;MAAEC,UAAU;MAAEC;IAAY,CAAC,GAAG,IAAI,CAACzE,gBAAgB,IAAI,CAAC,CAAC;IAC/F,MAAM;MAAE0E,KAAK;MAAEC,OAAO;MAAEC,gBAAgB;MAAEC,eAAe;MAAEC;IAAc,CAAC,GAAG,IAAI,CAAC7E,eAAe,IAAI,CAAC,CAAC;;IAEvG;IACA,OAAO;AACX,4DAA4DyE,KAAK,IAAI,mBAAmB;AACxF;AACA;AACA,QAAQL,IAAI,IAAI,SAAS;AACzB,OAAOC,GAAG,IAAI,SAAS;AACvB,wBAAwBC,mBAAmB,IAAI,cAAc;AAC7D,cAAcC,UAAU,IAAI,cAAc;AAC1C,eAAeC,WAAW,IAAI,cAAc;AAC5C;AACA;AACA,YAAY,CAAAE,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEI,QAAQ,KAAI,aAAa;AAC9C,QAAQ,CAAAJ,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEK,IAAI,KAAI,aAAa;AACtC,cAAc,CAAAL,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEM,UAAU,KAAI,aAAa;AAClD;AACA;AACA,EAAEL,gBAAgB,IAAI,gCAAgC;AACtD;AACA;AACA,EAAE,CAAAC,eAAe,aAAfA,eAAe,uBAAfA,eAAe,CAAEK,GAAG,CAACC,IAAI,IAAI;AAC/B,IAAIA,IAAI,CAACd,IAAI,KAAKc,IAAI,CAAC3E,IAAI,IAAI,KAAK,MAAM2E,IAAI,CAACC,WAAW,IAAI,iBAAiB;AAC/E,iCAAiCD,IAAI,CAACE,YAAY,IAAI,gBAAgB;AACtE,CAAC,CAAC,CAACC,IAAI,CAAC,EAAE,CAAC,KAAI,mDAAmD;AAClE;AACA;AACA,EAAER,aAAa,IAAI,uCAAuC;AAC1D;AACA;AACA,kMAAkMT,IAAI,IAAI,SAAS;AACnN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;EACC;;EAEA;AACF;AACA;AACA;EACEkB,iBAAiBA,CAAA,EAAG;IAClB;IACA,OAAOvD,IAAI,CAACwD,KAAK,CAACxD,IAAI,CAACC,SAAS,CAAC,IAAI,CAAClC,OAAO,CAAC,CAAC;EACjD;;EAEA;AACF;AACA;AACA;AACA;EACE0F,iBAAiBA,CAAC1F,OAAO,EAAE;IACxB;IACD,IAAI,CAAC2F,KAAK,CAACC,OAAO,CAAC5F,OAAO,CAAC,IAAIA,OAAO,CAAC6F,IAAI,CAACC,GAAG;MAAA,IAAAC,WAAA;MAAA,OAAI,CAACD,GAAG,IAAI,OAAOA,GAAG,CAACrF,IAAI,KAAK,QAAQ,IAAI,CAAC,CAAC,MAAM,EAAE,OAAO,CAAC,CAACuF,QAAQ,CAACF,GAAG,CAACrF,IAAI,CAAC,IAAI,CAACkF,KAAK,CAACC,OAAO,CAACE,GAAG,CAACpF,KAAK,CAAC,IAAI,GAAAqF,WAAA,GAACD,GAAG,CAACpF,KAAK,CAAC,CAAC,CAAC,cAAAqF,WAAA,eAAZA,WAAA,CAAcpF,IAAI;IAAA,EAAC,EAAE;MACnLR,OAAO,CAAC0C,KAAK,CAAC,6EAA6E,CAAC;MAC5F,MAAM,IAAI/C,KAAK,CAAC,4FAA4F,CAAC;IACjH;IACA;IACA,IAAI,CAACE,OAAO,GAAGiC,IAAI,CAACwD,KAAK,CAACxD,IAAI,CAACC,SAAS,CAAClC,OAAO,CAAC,CAAC;IAClDG,OAAO,CAACC,GAAG,CAAC,8BAA8B,IAAI,CAACJ,OAAO,CAACsD,MAAM,YAAY,CAAC;EAC5E;;EAEA;AACF;AACA;AACA;EACE2C,KAAKA,CAAA,EAAG;IACN9F,OAAO,CAACC,GAAG,CAAC,qDAAqD,CAAC;IAClE,MAAM8F,uBAAuB,GAAG,IAAI,CAACjG,gBAAgB;IACrD,MAAMkG,sBAAsB,GAAG,IAAI,CAACjG,eAAe;IACnD,IAAI,CAACF,OAAO,GAAG,EAAE;IACjB,IAAI,CAACC,gBAAgB,GAAG,IAAI;IAC5B,IAAI,CAACC,eAAe,GAAG,IAAI;IAE3B,IAAIgG,uBAAuB,IAAIC,sBAAsB,EAAE;MACnDhG,OAAO,CAACC,GAAG,CAAC,uDAAuD,CAAC;MACpE,IAAI,CAACC,UAAU,CAAC6F,uBAAuB,EAAEC,sBAAsB,CAAC;IACpE,CAAC,MAAM;MACHhG,OAAO,CAACC,GAAG,CAAC,0EAA0E,CAAC;IAC3F;EACF;AACF;AAEA,eAAeV,aAAa","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}